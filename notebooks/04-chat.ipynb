{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import APIRouter, HTTPException\n",
    "from api.config import OLLAMA_URL\n",
    "from api.schemas.chat import *\n",
    "from api.services.chat import *\n",
    "from api.services.llm_loaders import *\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import FewShotPromptTemplate\n",
    "\n",
    "# Instantiate router\n",
    "router = APIRouter(\n",
    "    prefix=\"/api/chat\", tags=[\"chat\"], responses={404: {\"description\": \"Not found\"}},\n",
    ")\n",
    "\n",
    "@router.post(\"/\", response_model=ChatResponse, status_code=200)\n",
    "async def main(request: ChatRequest):\n",
    "    memory = request.history\n",
    "\n",
    "    prompt_template = PromptTemplate.from_template(\"role: {role}\\ncontent: {content}\")\n",
    "\n",
    "    chat_prompt_template = FewShotPromptTemplate(\n",
    "        examples=memory,\n",
    "        example_prompt=prompt_template,\n",
    "        suffix=\"role: {role}\\ncontent: {content}\",\n",
    "        input_variables=[\"role\", \"content\"] \n",
    "    )\n",
    "\n",
    "    chat_prompt = chat_prompt_template.invoke(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": request.prompt\n",
    "        }\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        llm = load_llm_ollama(request.model, base_url=OLLAMA_URL)\n",
    "        response = llm.invoke(chat_prompt)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "    \n",
    "    return ChatResponse(history=memory, prompt=request.prompt, response=response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ai', 'Hello! How can I help you today?'), ('human', 'Hello, My name is John Doe'), ('ai', 'Hello, John Doe! How can I help you today?')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "memory = [\n",
    "    {\n",
    "        \"role\": \"ai\",\n",
    "        \"content\": \"Hello! How can I help you today?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"human\",\n",
    "        \"content\": \"Hello, My name is John Doe\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"ai\",\n",
    "        \"content\": \"Hello, John Doe! How can I help you today?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convert to list of tuples\n",
    "memory = [(entry[\"role\"], entry[\"content\"]) for entry in memory]\n",
    "\n",
    "print(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a chat assistant called LoLLa3, Local LLama3 Assistant.\"\n",
    "\n",
    "# Create a flat list with system prompt, memory messages, and the human input\n",
    "all_messages = [(\"system\", system_prompt)] + memory + [(\"human\", \"{input}\")]\n",
    "\n",
    "print(all_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a chat assistant called LoLLa3, Local LLama3 Assistant.'), AIMessage(content='Hello! How can I help you today?'), HumanMessage(content='Hello, My name is John Doe'), AIMessage(content='Hello, John Doe! How can I help you today?'), HumanMessage(content='Who am I?')]\n"
     ]
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(all_messages)\n",
    "\n",
    "prompt = \"Who am I?\"\n",
    "\n",
    "chat_prompt = chat_template.invoke(\n",
    "    {\n",
    "        \"input\": prompt\n",
    "    }\n",
    ")\n",
    "\n",
    "print(chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "llm = \"\"\n",
    "\n",
    "chain = chat_prompt | llm\n",
    "chain.invoke({\"input\": \"What is \"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = [\n",
    "    {\n",
    "        \"role\": \"ai\",\n",
    "        \"content\": \"Hello! How can I help you today?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"human\",\n",
    "        \"content\": \"Hello, My name is John Doe\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"ai\",\n",
    "        \"content\": \"Hello, John Doe! How can I help you today?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"{role}:{content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai:Hello! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template.invoke(memory[0]).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai:Hello! How can I help you today?\n",
      "\n",
      "human:Hello, My name is John Doe\n",
      "\n",
      "ai:Hello, John Doe! How can I help you today?\n",
      "\n",
      "role: user\n",
      "content: What is my name?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=memory,\n",
    "    example_prompt=prompt_template,\n",
    "    suffix=\"role: {role}\\ncontent: {content}\",\n",
    "    input_variables=[\"role\", \"content\"]\n",
    ")\n",
    "\n",
    "print(\n",
    "    prompt.invoke(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is my name?\"\n",
    "        }\n",
    "    ).to_string()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
