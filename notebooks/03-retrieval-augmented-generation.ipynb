{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fontana/Desktop/idp/cs-2024-01/trabalhos/trabalho-fp-2024-01/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain import hub\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import List, Dict, Any, Optional\n",
    "from typing import Any, Dict, List\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.vectorstores import VST\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.vectorstores import VectorStore, VectorStoreRetriever\n",
    "from typing import Any\n",
    "from langchain_core.language_models.llms import BaseLLM\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_document(file_path: str, **kwargs: Dict[str, Any]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Loads PDF document.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): File path\n",
    "        **kwargs: Keyword arguments\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: List of documents\n",
    "\n",
    "    Raises:\n",
    "        Exception: If failed to load PDF document\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Instantiate loader\n",
    "        loader = PyPDFLoader(file_path=file_path, **kwargs)\n",
    "        # Load document\n",
    "        document = loader.load()\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to load PDF document: {e}\")\n",
    "    return document\n",
    "\n",
    "\n",
    "def split_documents(documents: List[Document], chunk_size:int=1000, chunk_overlap:int=200, **kwargs) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks.\n",
    "\n",
    "    Args:\n",
    "        documents (List[Document]): List of documents\n",
    "        chunk_size (int): Size of the chunk\n",
    "        chunk_overlap (float): Overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        List[Document]: List of documents with chunks.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Instantiate text splitter\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size, chunk_overlap=chunk_overlap, add_start_index=True, **kwargs\n",
    "            )\n",
    "        # Split text\n",
    "        splits = text_splitter.split_documents(documents)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to split documents: {e}\")\n",
    "\n",
    "    return splits\n",
    "\n",
    "def load_embeddings_model_hf(model_name: Optional[str]=\"sentence-transformers/all-MiniLM-L6-v2\") -> Embeddings:\n",
    "    \"\"\"\n",
    "    load embeddings model.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Model name\n",
    "    \n",
    "    Returns:\n",
    "        Embeddings: Embeddings model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Instantiate embeddings\n",
    "        embeddings_model = HuggingFaceEmbeddings(model_name=model_name, show_progress=True)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to load embeddings model: {e}\")\n",
    "    \n",
    "    return embeddings_model\n",
    "\n",
    "def load_chroma_vectorstore(documents: List[Document], embeddings_model: Embeddings, **kwargs: Dict[str, Any]) -> VST:\n",
    "    \"\"\"\n",
    "    Loads documents into Chromadb.\n",
    "\n",
    "    Args:\n",
    "        documents (List[Document]): List of documents\n",
    "        embeddings_model (Embeddings): Embeddings model\n",
    "        **kwargs: Keyword arguments\n",
    "\n",
    "    Returns:\n",
    "        VST: Vector store\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load vectorstore\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=documents, embedding=embeddings_model, **kwargs\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to load vectorstore: {e}\")\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "def load_retriever(vectorstore: VectorStore, **kwargs: Dict[str, Any]) -> VectorStoreRetriever:\n",
    "    \"\"\"\n",
    "    Loads retriever from vectorstore.\n",
    "\n",
    "    Args:\n",
    "        vectorstore (VectorStore): Vector store\n",
    "\n",
    "    Returns:\n",
    "        VectorStoreRetriever: Vector store retriever\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load retriever\n",
    "        retriever = vectorstore.as_retriever(**kwargs)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to load retriever: {e}\")\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def load_llm_ollama(model_name:str='llama3:instruct', base_url:str=None, **kwargs) -> BaseLLM:\n",
    "    \"\"\"\n",
    "    Load large language model from Ollama.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the model to load\n",
    "        pipeline_kwargs Optional(Dict[str, Any]): The pipeline actions.\n",
    "\n",
    "    Returns:\n",
    "        BaseLLM: The loaded language model.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If there is an error loading the model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        llm = Ollama(model=model_name, base_url=base_url, **kwargs )\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading model {model_name}: {e}\") from e        \n",
    "    \n",
    "    return llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fontana/Desktop/idp/cs-2024-01/trabalhos/trabalho-fp-2024-01/venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/Users/fontana/Desktop/idp/cs-2024-01/trabalhos/trabalho-fp-2024-01/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Batches: 100%|██████████| 3/3 [00:02<00:00,  1.28it/s]\n"
     ]
    }
   ],
   "source": [
    "PATH = \"./data/responsible-use-guide-pdf.pdf\"\n",
    "\n",
    "# Load document\n",
    "documents = load_pdf_document(PATH)\n",
    "\n",
    "# Split document\n",
    "splits = split_documents(documents)\n",
    "\n",
    "# Load embeddings model\n",
    "embeddings_model = load_embeddings_model_hf()\n",
    "\n",
    "\n",
    "# Load vectorstore\n",
    "vectorstore = load_chroma_vectorstore(\n",
    "    documents=splits, embeddings_model=embeddings_model\n",
    ")\n",
    "\n",
    "# Load retriever\n",
    "retriever = load_retriever(vectorstore)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.78s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Responsible \\nUse Guide\\nResources and best practices for \\nresponsible development of products \\nbuilt with large language models\\nMeta Llama', metadata={'page': 0, 'source': './data/responsible-use-guide-pdf.pdf', 'start_index': 0}),\n",
       " Document(page_content='The recommendations included in this guide reflect \\ncurrent research on responsible generative AI. We \\nexpect these to evolve as the field advances and \\naccess to foundation models grows, inviting further \\ninnovation on AI safety. Decisions to implement \\nbest practices should be evaluated based on the \\njurisdiction where your products will be deployed and \\nshould follow your company’s internal legal and risk \\nmanagement processes.How to use this guide\\nThis guide is a resource for developers that outlines \\ncommon approaches to building responsibly at each \\nlevel of an LLM-powered product. It covers best \\npractices and considerations that developers should \\nevaluate in the context of their specific use case and \\nmarket. It also highlights some mitigation strategies \\nand resources available to developers to address risks \\nat various points in the system. These best practices \\nshould be considered holistically because strategies \\nadopted at one level can impact the entire system. \\n3', metadata={'page': 4, 'source': './data/responsible-use-guide-pdf.pdf', 'start_index': 0}),\n",
       " Document(page_content='principles and guidance on risk management released \\nby academic and expert institutions, such as: \\n• OECD’s AI Principles \\n• NIST’s Trustworthy and Responsible AI  \\nResource Center\\n7\\nApril 2024 AI at Meta', metadata={'page': 8, 'source': './data/responsible-use-guide-pdf.pdf', 'start_index': 1636}),\n",
       " Document(page_content='Contents\\n \\nOpen Innovation       1\\n How to use this guide      3\\nOverview of responsible AI & system design    4\\n Responsible AI considerations     4\\n Mitigation points for LLM-powered products 5\\nDevelopment of the foundation model     6\\nResponsible LLM product development stages    7\\n Determine use case       7\\n  Define content policies        8\\n  Understand alignment-helpfulness trade-offs    8\\n Model-level alignment        9\\n Step 1: Prepare data   10\\n Step 2: Train the model      11\\n  Reinforcement Learning from Human Feedback (RLHF)  12\\n  Reinforcement Learning from AI Feedback (RLAIF)                12\\n Step 3: Evaluate and improve performance    12\\n  Red teaming best practices    13\\n  Privacy adversarial attacks    14\\n System-level alignment     14\\n  Mitigating risks at the input level    15\\n  Mitigating risks at the output level  16\\n Evaluate effectiveness      17\\n  Build transparency and reporting mechanisms  \\n in user interactions                      17', metadata={'page': 1, 'source': './data/responsible-use-guide-pdf.pdf', 'start_index': 0})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT = \"How to use the guide\"\n",
    "\n",
    "# Load documents\n",
    "docs = retriever.invoke(PROMPT)\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Responsible \\nUse Guide\\nResources and best practices for \\nresponsible development of products \\nbuilt with large language models\\nMeta Llama\\n\\nThe recommendations included in this guide reflect \\ncurrent research on responsible generative AI. We \\nexpect these to evolve as the field advances and \\naccess to foundation models grows, inviting further \\ninnovation on AI safety. Decisions to implement \\nbest practices should be evaluated based on the \\njurisdiction where your products will be deployed and \\nshould follow your company’s internal legal and risk \\nmanagement processes.How to use this guide\\nThis guide is a resource for developers that outlines \\ncommon approaches to building responsibly at each \\nlevel of an LLM-powered product. It covers best \\npractices and considerations that developers should \\nevaluate in the context of their specific use case and \\nmarket. It also highlights some mitigation strategies \\nand resources available to developers to address risks \\nat various points in the system. These best practices \\nshould be considered holistically because strategies \\nadopted at one level can impact the entire system. \\n3\\n\\nprinciples and guidance on risk management released \\nby academic and expert institutions, such as: \\n• OECD’s AI Principles \\n• NIST’s Trustworthy and Responsible AI  \\nResource Center\\n7\\nApril 2024 AI at Meta\\n\\nContents\\n \\nOpen Innovation       1\\n How to use this guide      3\\nOverview of responsible AI & system design    4\\n Responsible AI considerations     4\\n Mitigation points for LLM-powered products 5\\nDevelopment of the foundation model     6\\nResponsible LLM product development stages    7\\n Determine use case       7\\n  Define content policies        8\\n  Understand alignment-helpfulness trade-offs    8\\n Model-level alignment        9\\n Step 1: Prepare data   10\\n Step 2: Train the model      11\\n  Reinforcement Learning from Human Feedback (RLHF)  12\\n  Reinforcement Learning from AI Feedback (RLAIF)                12\\n Step 3: Evaluate and improve performance    12\\n  Red teaming best practices    13\\n  Privacy adversarial attacks    14\\n System-level alignment     14\\n  Mitigating risks at the input level    15\\n  Mitigating risks at the output level  16\\n Evaluate effectiveness      17\\n  Build transparency and reporting mechanisms  \\n in user interactions                      17'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use retrieved documents as context\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: \\nAnswer the question based only on the following context:\\n\\nResponsible \\nUse Guide\\nResources and best practices for \\nresponsible development of products \\nbuilt with large language models\\nMeta Llama\\n\\nThe recommendations included in this guide reflect \\ncurrent research on responsible generative AI. We \\nexpect these to evolve as the field advances and \\naccess to foundation models grows, inviting further \\ninnovation on AI safety. Decisions to implement \\nbest practices should be evaluated based on the \\njurisdiction where your products will be deployed and \\nshould follow your company’s internal legal and risk \\nmanagement processes.How to use this guide\\nThis guide is a resource for developers that outlines \\ncommon approaches to building responsibly at each \\nlevel of an LLM-powered product. It covers best \\npractices and considerations that developers should \\nevaluate in the context of their specific use case and \\nmarket. It also highlights some mitigation strategies \\nand resources available to developers to address risks \\nat various points in the system. These best practices \\nshould be considered holistically because strategies \\nadopted at one level can impact the entire system. \\n3\\n\\nprinciples and guidance on risk management released \\nby academic and expert institutions, such as: \\n• OECD’s AI Principles \\n• NIST’s Trustworthy and Responsible AI  \\nResource Center\\n7\\nApril 2024 AI at Meta\\n\\nContents\\n \\nOpen Innovation       1\\n How to use this guide      3\\nOverview of responsible AI & system design    4\\n Responsible AI considerations     4\\n Mitigation points for LLM-powered products 5\\nDevelopment of the foundation model     6\\nResponsible LLM product development stages    7\\n Determine use case       7\\n  Define content policies        8\\n  Understand alignment-helpfulness trade-offs    8\\n Model-level alignment        9\\n Step 1: Prepare data   10\\n Step 2: Train the model      11\\n  Reinforcement Learning from Human Feedback (RLHF)  12\\n  Reinforcement Learning from AI Feedback (RLAIF)                12\\n Step 3: Evaluate and improve performance    12\\n  Red teaming best practices    13\\n  Privacy adversarial attacks    14\\n System-level alignment     14\\n  Mitigating risks at the input level    15\\n  Mitigating risks at the output level  16\\n Evaluate effectiveness      17\\n  Build transparency and reporting mechanisms  \\n in user interactions                      17\\n\\n---\\n\\nAnswer the question based on the above context: How to use the guide\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Generate prompt\n",
    "prompt_template = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "rag_prompt = prompt_template.format(context=context, question=PROMPT)\n",
    "\n",
    "rag_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
