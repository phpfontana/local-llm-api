{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Milvus\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain import hub\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import List, Dict, Any, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LangChainCollection']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "client = MilvusClient(host='localhost', port='19530')\n",
    "client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fontana/Desktop/idp/cs-2024-01/trabalhos/trabalho-fp-2024-01/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.embeddings import Embeddings\n",
    "from typing import Optional\n",
    "\n",
    "def load_embeddings_model_hf(model_name: Optional[str]) -> Embeddings:\n",
    "    \"\"\"\n",
    "    load embeddings model.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Model name\n",
    "    \n",
    "    Returns:\n",
    "        Embeddings: Embeddings model\n",
    "    \"\"\"\n",
    "\n",
    "    # Instantiate embeddings\n",
    "    embeddings_model = HuggingFaceEmbeddings(\n",
    "        model_name=model_name        \n",
    "    )\n",
    "\n",
    "    return embeddings_model\n",
    "\n",
    "def generate_document_embeddings(documents: List[str], embeddings_model: Embeddings) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Embed documents.\n",
    "\n",
    "    Args:\n",
    "        documents (List[str]): List of documents\n",
    "        embeddings_model (Embeddings): Embeddings model\n",
    "    \n",
    "    Returns:\n",
    "        List[List[float]]: List of embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed documents\n",
    "    embeddings = embeddings_model.embed_documents(documents)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def generate_query_embeddings(query: str, embeddings_model: Embeddings) -> List[float]:\n",
    "    \"\"\"\n",
    "    Embed query.\n",
    "\n",
    "    Args:\n",
    "        query (str): Query\n",
    "        embeddings_model (Embeddings): Embeddings model\n",
    "    \n",
    "    Returns:\n",
    "        List[float]: Embedding\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed query\n",
    "    embedding = embeddings_model.embed_query(query)\n",
    "\n",
    "    return embedding\n",
    "\n",
    "model_name = \"all-mpnet-base-v2\"\n",
    "\n",
    "# Load embeddings model\n",
    "embeddings_model = load_embeddings_model(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trabalho-fp-2024-01\n",
      "\n",
      "Trabalho FÃ¡brica de Projetos 02 - 2024/01\n",
      "\n",
      "Description\n",
      "\n",
      "This is a full scale Retrieval Augmented Generation LLM server built using LangChain, Milvus, and HugginhFace.\n",
      "\n",
      "API\n",
      "\n",
      "Instantiate Milvus vectorstore, connect to milvus client. create a collection.\n",
      "\n",
      "Generate\n",
      "\n",
      "Loads an LLM model locally and generates a response based on a prompt\n",
      "\n",
      "request body example\n",
      "{\n",
      "  \"model\": \"llama3\",\n",
      "  \"prompt\": \"Hello World\",\n",
      "  \"stream\": false,\n",
      "  \"options\": {}\n",
      "}\n",
      "\n",
      "response\n",
      "{\n",
      "  \"model\": \"llama3\",\n",
      "  \"created_at\": \"2024-06-13 15:28:24\",\n",
      "  \"response\": \"Hello World! Nice to meet you! What brings you here today?\",\n",
      "  \"total duration\": 5.958862066268921\n",
      "}\n",
      "\n",
      "Embeddings\n",
      "\n",
      "@load_documents\n",
      "- load document\n",
      "- split document based on markdown\n",
      "- store with HF embeddings model\n",
      "\n",
      "@retrieve_documents\n",
      "- instantiate milvus langchain client\n",
      "- as retriever + embedding\n",
      "- query\n",
      "\n",
      "https://python.langchain.com/v0.2/docs/integrations/vectorstores/milvus/\n",
      "\n",
      "@rag\n",
      "https://github.com/stephen37/Milvus_demo/blob/main/milvus_rag/rag_milvus_ollama.ipynb\n",
      "\n",
      "/api/generate/\n",
      "\n",
      "request body example\n",
      "{\n",
      "  \"model\": \"llama3\",\n",
      "  \"prompt\": \"Hello World\",\n",
      "  \"stream\": false,\n",
      "  \"options\": {}\n",
      "}\n",
      "\n",
      "response\n",
      "{\n",
      "  \"model\": \"llama3\",\n",
      "  \"created_at\": \"2024-06-13 15:28:24\",\n",
      "  \"response\": \"Hello World! Nice to meet you! What brings you here today?\",\n",
      "  \"total duration\": 5.958862066268921\n",
      "}\n",
      "{'source': './data/README.md'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_core.documents import Document\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def load_markdown_document(file_path: str, **kwargs: Dict[str, Any]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Loads markdown document.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): File path\n",
    "        **kwargs: Keyword arguments\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: List of documents\n",
    "\n",
    "    Raises:\n",
    "        Exception: If failed to load markdown document\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Instantiate loader\n",
    "        loader = UnstructuredMarkdownLoader(file_path=file_path, **kwargs)\n",
    "        # Load document\n",
    "        document = loader.load()\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to load markdown document: {e}\")\n",
    "    return document\n",
    "\n",
    "def load_pdf_document(file_path: str, **kwargs: Dict[str, Any]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Loads PDF document.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): File path\n",
    "        **kwargs: Keyword arguments\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: List of documents\n",
    "\n",
    "    Raises:\n",
    "        Exception: If failed to load PDF document\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Instantiate loader\n",
    "        loader = PyPDFLoader(file_path=file_path, **kwargs)\n",
    "        # Load document\n",
    "        document = loader.load()\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to load PDF document: {e}\")\n",
    "    return document\n",
    "\n",
    "MARKDOWN_PATH = \"./data/README.md\"\n",
    "\n",
    "\n",
    "# Load markdown document\n",
    "document = load_markdown_document(file_path=MARKDOWN_PATH)\n",
    "\n",
    "print(document[0].page_content)\n",
    "print(document[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from typing import List, Tuple\n",
    "\n",
    "def split_markdown_text(text: str, headers_to_split_on: List[Tuple[str, str]], **kwargs: Dict[str, Any]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split markdown document into sections.\n",
    "\n",
    "    Args:\n",
    "        document (List[Document]): List of documents\n",
    "        headers_to_split_on (List[Tuple[str, str]]): Headers to split on\n",
    "        **kwargs: Keyword arguments\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: List of documents\n",
    "\n",
    "    Raises:\n",
    "        Exception: If failed to split markdown document\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Instantiate markdown splitter\n",
    "        markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "            headers_to_split_on=headers_to_split_on, **kwargs\n",
    "            )\n",
    "        # Split text\n",
    "        splits = markdown_splitter.split_text(text)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to split markdown document: {e}\")\n",
    "\n",
    "    return splits\n",
    "\n",
    "\n",
    "def split_documents(documents: List[Document], chunk_size:int=1000, chunk_overlap:int=200) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks.\n",
    "\n",
    "    Args:\n",
    "        documents (List[Document]): List of documents\n",
    "        chunk_size (int): Size of the chunk\n",
    "        chunk_overlap (float): Overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        List[Document]: List of documents with chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    # Instantiate text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, add_start_index=True\n",
    "        )\n",
    "\n",
    "    # Split text\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "\n",
    "    return splits\n",
    "\n",
    "# Headers to split on\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "kwargs = {\n",
    "    \"strip_headers\": True\n",
    "}\n",
    "\n",
    "# Split markdown documents\n",
    "splits = split_markdown_document(document, headers_to_split_on, **kwargs)\n",
    "\n",
    "print(len(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='def hello_world():\\n    print(\"Hello, World!\")'),\n",
       " Document(page_content='# Call the function\\nhello_world()')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "PYTHON_CODE = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "# Call the function\n",
    "hello_world()\n",
    "\"\"\"\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    ")\n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "python_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import VectorStore, VectorStoreRetriever, VST\n",
    "\n",
    "def load_milvus_vectorstore():\n",
    "\n",
    "    pass\n",
    "\n",
    "def load_vectorstore(vectorstore: VectorStore, documents: List[Document], embeddings_model: Embeddings, **kwargs: Dict[str, Any]) -> VST:\n",
    "    \"\"\"\n",
    "    Loads documents into vectorstore.\n",
    "\n",
    "    Args:\n",
    "        vectorstore (VectorStore): Vector store\n",
    "        documents (List[Document]): List of documents\n",
    "        embeddings_model (Embeddings): Embeddings model\n",
    "        **kwargs: Keyword arguments\n",
    "\n",
    "    Returns:\n",
    "        VST: Vector store\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load vectorstore\n",
    "        vectorstore = vectorstore.from_documents(\n",
    "            documents=documents, embedding=embeddings_model, **kwargs\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to load vectorstore: {e}\")\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "def load_retriever(vectorstore: VectorStore) -> VectorStoreRetriever:\n",
    "    \"\"\"\n",
    "    Loads retriever from vectorstore.\n",
    "\n",
    "    Args:\n",
    "        vectorstore (VectorStore): Vector store\n",
    "\n",
    "    Returns:\n",
    "        VectorStoreRetriever: Vector store retriever\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load retriever\n",
    "        retriever = vectorstore.as_retriever()\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to load retriever: {e}\")\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "URI = \"http://localhost:19530\"\n",
    "\n",
    "# Load vectorstore\n",
    "vectorstore = Milvus.from_documents(\n",
    "    documents=splits, embedding=embeddings_model,\n",
    "    connection_args={\"uri\": URI}\n",
    ")\n",
    "\n",
    "# Load retriever\n",
    "retriever = vectorstore.as_retriever() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.language_models.llms import BaseLLM\n",
    "\n",
    "def load_llm_ollama(model_name: str, base_url, pipeline_kwargs: Optional[Dict[str, Any]]=None) -> BaseLLM:\n",
    "    \"\"\"\n",
    "    Load large language model from Ollama.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the model to load\n",
    "        pipeline_kwargs Optional(Dict[str, Any]): The pipeline actions.\n",
    "\n",
    "    Returns:\n",
    "        BaseLLM: The loaded language model.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If there is an error loading the model\n",
    "    \"\"\"\n",
    "    if pipeline_kwargs is None:\n",
    "        pipeline_kwargs = {}\n",
    "\n",
    "    try:\n",
    "        return Ollama(model=model_name, base_url=base_url, **pipeline_kwargs)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading model: {e}\")\n",
    "    \n",
    "\n",
    "model_name = \"llama3:instruct\"\n",
    "base_url = \"https://localhost:11434\"\n",
    "pipeline_kwargs = {\"stop\": \"<|eot_id|>\"}\n",
    "\n",
    "# Load LLM\n",
    "llm = load_llm_ollama(model_name, base_url, pipeline_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough(), }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know. The provided context does not indicate the purpose of a specific document, but rather appears to be code snippets or instructions for using Milvus and LLM RAG Server."
     ]
    }
   ],
   "source": [
    "prompt = \"What is the purpose of the document?\"\n",
    "\n",
    "for chunk in rag_chain.stream(prompt):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/stephen37/Milvus_demo/blob/main/milvus_rag/rag_milvus_ollama.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
