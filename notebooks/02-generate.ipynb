{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from langchain_core.language_models.llms import BaseLLM\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "def load_llm_hf(model_id: str, task: str, **kwargs) -> BaseLLM:\n",
    "    \"\"\"\n",
    "    Load language model.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Model name\n",
    "        task (str): Task\n",
    "        kwargs (Dict[str, Any]): Additional arguments\n",
    "\n",
    "    Returns:\n",
    "        BaseLLM: The loaded language model.\n",
    "    \"\"\"\n",
    " \n",
    "    # Load model tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "    # Load pipeline\n",
    "    pipe = pipeline(\n",
    "        task=task,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    # Instantiate LLM\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "    return llm\n",
    "\n",
    "def load_llm_ollama(model_name: str, base_url: str, **kwargs) -> BaseLLM:\n",
    "    \"\"\"\n",
    "    Load large language model from Ollama.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the model to load\n",
    "        pipeline_kwargs Optional(Dict[str, Any]): The pipeline actions.\n",
    "\n",
    "    Returns:\n",
    "        BaseLLM: The loaded language model.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If there is an error loading the model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        llm = Ollama(model=model_name, base_url=base_url, **kwargs )\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading model {model_name}: {e}\") from e        \n",
    "    \n",
    "    return llm\n",
    "    \n",
    "async def generate_response(prompt: str, llm: BaseLLM) -> Any:\n",
    "    \"\"\"\n",
    "    Generate a response using large language model.\n",
    "\n",
    "    Args:\n",
    "        prompt (String): The user prompt.\n",
    "        llm (BaseLLM): The loaded language model.\n",
    "\n",
    "    Returns:\n",
    "        Any: The generated response or a streaming response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return llm.invoke(prompt)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error generating: {str(e)}\") from e\n",
    "\n",
    "\n",
    "async def generate_streaming_response(prompt: str, llm: BaseLLM) -> Any:\n",
    "    \"\"\"\n",
    "    Generate a response using large language model.\n",
    "\n",
    "    Args:\n",
    "        promt (String): The llm prompt.\n",
    "        llm (BaseLLM): The loaded language model.\n",
    "\n",
    "    Returns:\n",
    "        Any: The generated streaming response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for chunks in llm.stream(prompt):\n",
    "            yield chunks\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error generating: {str(e)}\") from e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"llama3:instruct\"\n",
    "BASE_URL = \"http://localhost:11434\"\n",
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "llm = load_llm_ollama(model_name, base_url=BASE_URL)\n",
    "\n",
    "response = await generate_response(prompt, llm)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Any, List\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class EmbeddingsRequest(BaseModel):\n",
    "    model: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    query: Optional[str] = None\n",
    "\n",
    "class EmbeddingsResponse(BaseModel):\n",
    "    embeddings: List[float]\n",
    "\n",
    "class GenerateRequest(BaseModel):\n",
    "    model: str = \"llama3\"\n",
    "    prompt: Optional[str] = None\n",
    "\n",
    "class GenerateResponse(BaseModel):\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'async for' outside async function (1359228001.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[21], line 13\u001b[0;36m\u001b[0m\n\u001b[0;31m    async for response_chunk in response:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'async for' outside async function\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "def main():\n",
    "    model_name = \"llama3:instruct\"\n",
    "    BASE_URL = \"http://localhost:11434\"\n",
    "    prompt = \"What is the capital of France?\"\n",
    "\n",
    "    llm = load_llm_ollama(model_name, base_url=BASE_URL)\n",
    "\n",
    "    response = generate_streaming_response(prompt, llm)\n",
    "\n",
    "    # Process the streaming response\n",
    "    async for response_chunk in response:\n",
    "        print(response_chunk)\n",
    "\n",
    "\n",
    "# Run the main function\n",
    "asyncio.run(main())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main(request: ModelRequest):\n",
    "    \"\"\"\n",
    "    Main endpoint to generate text using a language model.\n",
    "\n",
    "    Args:\n",
    "        request (ModelRequest): The request containing the model, prompt, and options\n",
    "\n",
    "    Returns:\n",
    "        Any: The generated response or a streaming response\n",
    "    \"\"\"\n",
    "    model_name = request.model\n",
    "    llm = load_llm_ollama(model_name, request.options)\n",
    "    if request.stream:\n",
    "        return StreamingResponse(generate_streaming_response(request.prompt, llm), media_type=\"text/event-stream\")\n",
    "    else:\n",
    "        created_at = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        start = time.time()\n",
    "        response = await generate_response(request.prompt, llm)\n",
    "        end = time.time()\n",
    "        return JSONResponse(\n",
    "            content={\n",
    "                \"model\": model_name,\n",
    "                \"created_at\": created_at,\n",
    "                \"response\": response,\n",
    "                \"total duration\": end - start,\n",
    "            })\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
